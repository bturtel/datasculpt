{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import toml\n",
    "from openai import OpenAI\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from IPython.display import HTML, display\n",
    "import numpy as np\n",
    "\n",
    "secrets = toml.load(\"secrets.toml\")\n",
    "openai_key = secrets[\"OPEN_AI_KEY\"]\n",
    "REDDIT = praw.Reddit(\n",
    "    client_id=secrets[\"reddit\"][\"client_id\"],\n",
    "    client_secret=secrets[\"reddit\"][\"client_secret\"],\n",
    "    user_agent=secrets[\"reddit\"][\"user_agent\"]\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_USE_CASE = \"\"\"\n",
    "We are analyzing Reddit posts to understand how people are using AI and chatbots for mental health, coaching, or emotional support.\n",
    "Specifically, we want to identify posts where users share their personal experiences using AI tools for:\n",
    "- Managing mental health conditions (anxiety, depression, ADHD, OCD, PTSD, trauma, etc.)\n",
    "- Emotional support and wellbeing\n",
    "- Therapy supplements or alternatives\n",
    "- Wellness coaching and goal setting\n",
    "- Help focusing, goal setting, managing stress, overcoming obstacles, etc.\n",
    "- Other similar use cases for AI in mental health\n",
    "\n",
    "The post should include first-hand experience using AI tools, not just general discussion about AI in mental health.\n",
    "This does NOT need to be the main focus of the post, but it should clearly mention using AI for the use case described.\n",
    "We want to extract structured data about their experiences, including benefits, challenges, and specific use cases.\n",
    "Do NOT make stuff up.  ONLY use keywords that accurately fit what the schema describes. \n",
    "A keyword that applies to the post generally but not specifically to what is asked for by the schema should not be used.\n",
    "\"\"\"\n",
    "\n",
    "FIELDS = {\n",
    "    \"relevant_sample\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"Boolean indicating if text describes personal experience using AI for the use case described in the prompt\"\n",
    "    },\n",
    "    \"relevant_sample_explanation\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Explanation of why the sample was classified as relevant or not relevant\"\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "        \"type\": \"integer\",\n",
    "        \"description\": \"Integer 1-10 indicating sentiment TOWARDS using AI for mental health (10 most positive).  This is NOT sentiment of the post overall, just sentiment towards the interaction with AI.\"\n",
    "    },\n",
    "    \"benefits\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"List of keywords relating to perceived benefits of using the AI, e.g.: non_judgemental, on_demand, affordable, accessible, anonymous, consistent, supportive, patient\"\n",
    "    },\n",
    "    \"downsides\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"List of keywords relating to downsides of using the AI, e.g.: repetitive, robotic, shallow, unreliable, addictive, avoidant, limited\"\n",
    "    },\n",
    "    \"use_cases\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"List of keywords relating to how AI is used, e.g.: reflection, venting, self_talk, planning, CBT, journaling, motivation, reminders, emotional_support\"\n",
    "    },\n",
    "    \"conditions\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"List of keywords describing conditions being addressed, e.g.: ADHD, depression, anxiety, addiction, OCD, PTSD, bipolar, eating_disorder\"\n",
    "    },\n",
    "    \"seeing_provider\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"Boolean indicating if subject indicates they are CURRENTLY seeing a therapist or mental health provider\"\n",
    "    },\n",
    "    \"previous_provider\": {\n",
    "        \"type\": \"boolean\",\n",
    "        \"description\": \"Boolean indicating if subject indicates they have EVER seen a therapist or mental health provider\"\n",
    "    },\n",
    "    \"provider_problems\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"List of keywords relating to perceived issues with HUMAN PROVIDERS, e.g.: expensive, unavailable, inaccessible, scheduling, inconsistent, judgmental\"\n",
    "    },\n",
    "    \"fields_explanation\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Concise but thorough explanation of your reasoning for each field in the schema (except for relevant_sample and relevant_sample_explanation)\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Reddit search docs](https://support.reddithelp.com/hc/en-us/articles/19696541895316-Available-search-features)\n",
    "- [PRAW docs](https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = (\n",
    "    \"ADHD, Advice, Adulting, Alcoholism, Anger, Anxiety, AsianParentStories, \"\n",
    "    \"aspergirls, BipolarReddit, BlackMentalHealth, bodyacceptance, bpd, \"\n",
    "    \"careerguidance, CPTSD, dating_advice, dbtselfhelp, \"\n",
    "    \"DecidingToBeBetter, depression, depression_help, EDAnonymous, Enneagram, \"\n",
    "    \"GetMotivated, HealthAnxiety, Healthygamergg, hopefulmentalhealth, \"\n",
    "    \"lawofattraction, LucidDreaming, malementalhealth, meditation, \"\n",
    "    \"mental, mentalhealth, mentalhealthadvice, \"\n",
    "    \"mentalhealthph, mentalhealthsupport, mentalhealthuk, \"\n",
    "    \"mentalillness, MensMentalHealth, microdosing, \"\n",
    "    \"MMFB, nofap, nosurf, OCD, offmychest, pornfree, productivity, \"\n",
    "    \"Psychiatry, psychology, ptsd, QAnonCasualties, \"\n",
    "    \"raisedbynarcissists, relationship_advice, relationships, \"\n",
    "    \"selfimprovement, socialanxiety, socialskills, StopSmoking, Stress, \"\n",
    "    \"suicidewatch, TalkTherapy, teenagers, therapy, therapists, \"\n",
    "    \"traumatoolbox, TrueOffMyChest, WellnessPT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split subreddits string into list\n",
    "subreddit_list = [s.strip() for s in subreddits.split(',')]\n",
    "\n",
    "# Query for AI content in each subreddit\n",
    "posts = []\n",
    "query = '(AI OR \"artificial intelligence\" OR chatbot OR gpt OR chatGPT or Claude OR characterAI OR Gemini OR Woebot OR Wysa OR Youper Or Sintelly)'\n",
    "\n",
    "for subreddit in tqdm(subreddit_list):\n",
    "    try:\n",
    "        # Try to get the subreddit\n",
    "        sub = REDDIT.subreddit(subreddit)\n",
    "        \n",
    "        # Check if subreddit exists and has reasonable activity\n",
    "        try:\n",
    "            subscribers = sub.subscribers\n",
    "            if subscribers < 1000:\n",
    "                print(f\"Warning: {subreddit} has only {subscribers} subscribers\")\n",
    "                continue\n",
    "        except:\n",
    "            print(f\"Warning: Could not access subscriber count for {subreddit}\")\n",
    "            continue\n",
    "            \n",
    "        # Search within this subreddit\n",
    "        search_results = sub.search(\n",
    "            query,\n",
    "            sort='relevance',\n",
    "            time_filter='year',\n",
    "            limit=100\n",
    "        )\n",
    "        \n",
    "        # Add posts from this subreddit\n",
    "        for post in search_results:\n",
    "            posts.append({\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'score': post.score,\n",
    "                'created_utc': post.created_utc,\n",
    "                'id': post.id,\n",
    "                'subreddit': post.subreddit.display_name,\n",
    "                'url': f\"https://reddit.com{post.permalink}\",\n",
    "                'num_comments': post.num_comments\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing subreddit {subreddit}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Convert to dataframe\n",
    "subreddits_df = pd.DataFrame(posts)\n",
    "print(f\"\\nFound {len(subreddits_df)} total posts across all subreddits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "(title:AI OR title:\"artificial intelligence\" OR title:chatbot OR title:gpt OR title:Claude OR title:characterAI OR title:Gemini) AND \n",
    "(title:therapy OR title:therapist OR title:\"mental health\" OR title:anxiety OR title:adhd OR title:depression OR title:stress OR title:ocd OR title:relationships)\n",
    "\"\"\"\n",
    "posts = []\n",
    "search_results = REDDIT.subreddit(\"all\").search(\n",
    "    query,\n",
    "    sort='relevance',\n",
    "    syntax='lucene',\n",
    "    time_filter='year',\n",
    "    limit=10000\n",
    ")\n",
    "\n",
    "for post in search_results:\n",
    "    posts.append({\n",
    "        'title': post.title,\n",
    "        'text': post.selftext,\n",
    "        'score': post.score,\n",
    "        'created_utc': post.created_utc,\n",
    "        'id': post.id,\n",
    "        'subreddit': post.subreddit.display_name,\n",
    "        'url': f\"https://reddit.com{post.permalink}\",\n",
    "        'num_comments': post.num_comments\n",
    "    })\n",
    "    \n",
    "search_df = pd.DataFrame(posts)\n",
    "print(f\"Found {len(search_df)} posts\")\n",
    "print(\"\\nSample titles:\")\n",
    "print(search_df[['title', 'subreddit', 'score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine posts from both sources and deduplicate\n",
    "all_posts = pd.concat([subreddits_df, search_df], ignore_index=True)\n",
    "\n",
    "# Drop duplicates based on post ID since that's unique per Reddit post\n",
    "all_posts = all_posts.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "print(f\"Total posts after combining and deduping: {len(all_posts)}\")\n",
    "print(f\"Posts from subreddit search: {len(subreddits_df)}\")\n",
    "print(f\"Posts from keyword search: {len(search_df)}\")\n",
    "print(f\"Duplicates removed: {len(subreddits_df) + len(search_df) - len(all_posts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_posts(df):\n",
    "    \"\"\"Remove duplicate posts with same title/text, keeping the one with most comments\"\"\"\n",
    "    print(f\"Posts before deduplication: {len(df)}\")\n",
    "    \n",
    "    # Group by title and text to find duplicates\n",
    "    duplicates = df.groupby(['title', 'text']).agg({\n",
    "        'num_comments': 'max',  # Keep post with most comments\n",
    "        'id': 'count'  # Count occurrences\n",
    "    }).reset_index()\n",
    "\n",
    "    # Filter to only groups with duplicates\n",
    "    duplicates = duplicates[duplicates['id'] > 1]\n",
    "\n",
    "    # For each duplicate group, keep only the post with most comments\n",
    "    if len(duplicates) > 0:\n",
    "        for _, dup in duplicates.iterrows():\n",
    "            # Find all posts with this title/text\n",
    "            mask = (df['title'] == dup['title']) & (df['text'] == dup['text'])\n",
    "            # Keep only the one with most comments\n",
    "            to_drop = df[mask & (df['num_comments'] < dup['num_comments'])].index\n",
    "            df = df.drop(to_drop)\n",
    "\n",
    "    print(f\"Posts after deduplication: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "all_posts = deduplicate_posts(all_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_post_relevance(post, use_case):\n",
    "    # Analyze post relevance\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"Post Title: {post['title']}\n",
    "Post Content: {post.get('text', '[No content]')}\n",
    "Use Case: {use_case}\n",
    "Question: Is this post relevant to our use case? Please answer with a brief 'Yes' or 'No' and short explanation.\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Determine boolean based on start of response\n",
    "    is_relevant = None\n",
    "    if response_text.strip().lower().startswith('yes'):\n",
    "        is_relevant = True\n",
    "    elif response_text.strip().lower().startswith('no'):\n",
    "        is_relevant = False\n",
    "        \n",
    "    return (is_relevant, response_text)\n",
    "\n",
    "use_case = \"\"\"\n",
    "We are looking for posts relating to how people are using AI, chatbots, or virtual companions for mental health support or coaching.\n",
    "This can include for any purpose: anxiety, adhd, depression, stress, ocd, relationships, goal setting, wellbeing, etc.\n",
    "To qualify, the user must discuss their own experience using one of these tools, not just discussing in abstract or commenting on the use of AI in general.\n",
    "\"\"\"\n",
    "\n",
    "print_posts = False\n",
    "from tqdm import tqdm\n",
    "for idx, post in tqdm(all_posts.iterrows(), total=len(all_posts)):\n",
    "    # Skip if already analyzed\n",
    "    if pd.notna(post.get('is_relevant')):\n",
    "        continue\n",
    "        \n",
    "    is_relevant, analysis = analyze_post_relevance(post, use_case)\n",
    "    \n",
    "    # Add relevance fields to existing post\n",
    "    all_posts.at[idx, 'is_relevant'] = is_relevant\n",
    "    all_posts.at[idx, 'relevant_explanation'] = analysis\n",
    "    \n",
    "    # Print concise results\n",
    "    if print_posts:\n",
    "        print(f\"\\n[{post['subreddit']}] {post['title']}\")\n",
    "        print(f\"Link: {post['url']}\")\n",
    "        print(f\"Analysis: {analysis}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nRelevant Post Counts:\\n\", all_posts['is_relevant'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant posts\n",
    "# samples = pd.read_json('relevant_posts.json')  # To Reload\n",
    "samples = all_posts[all_posts['is_relevant'] == True].drop(columns=['is_relevant', 'relevant_explanation']).copy()\n",
    "samples['created_utc'] = pd.to_datetime(samples['created_utc'], unit='s')\n",
    "samples.to_json('samples.json', orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Fields with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "def extract_fields(text: str, fields: Dict[str, dict], prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract structured fields from text using OpenAI\"\"\"\n",
    "    schema = {\n",
    "        \"name\": \"extract_fields\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": fields,\n",
    "            \"required\": list(fields.keys()),\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": schema\n",
    "        }\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "prompt=ANALYSIS_USE_CASE + \"\\n\\nAnalyze the following Reddit post and extract the requested fields, carefully, according to the schema.  Explain your reasoning in the last field.\"\n",
    "\n",
    "# Store results in a dictionary\n",
    "results = {}\n",
    "\n",
    "# Loop through all samples and extract fields\n",
    "for idx, sample in tqdm(samples.iterrows(), total=len(samples), desc=\"Extracting fields\"):\n",
    "    text = f\"{sample['title']}\\n\\n{sample['text']}\"\n",
    "    result = extract_fields(text, FIELDS, prompt)\n",
    "    results[idx] = result\n",
    "\n",
    "# Convert results dictionary to dataframe and merge with original\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "samples = samples.merge(results_df, left_index=True, right_index=True)\n",
    "samples.to_json('samples-with-fields-4o.json', orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back in\n",
    "samples = pd.read_json('samples-with-fields-4o.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter 2nd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_counts = samples['relevant_sample'].value_counts()\n",
    "print(\"Breakdown of relevant_sample vs not:\")\n",
    "print(relevant_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_irrelevant_samples(df):\n",
    "    # Create combined text field\n",
    "    df['combined_text'] = '[' + df['subreddit'] + '] ' + \\\n",
    "                         df['title'] + ' : ' + \\\n",
    "                         df['text']\n",
    "\n",
    "    # Create relevance field                                    \n",
    "    df['relevance_info'] = df['relevant_sample'].astype(str) + \\\n",
    "                          ' : ' + df['relevant_sample_explanation']\n",
    "\n",
    "    display(df[['combined_text', 'relevance_info']].style.set_properties(**{\n",
    "        'white-space': 'pre-wrap',\n",
    "        'text-align': 'left'\n",
    "    }).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'left')]},\n",
    "        {'selector': '', 'props': [('border', '1px solid grey')]}\n",
    "    ]))\n",
    "\n",
    "display_irrelevant_samples(samples[~samples['relevant_sample']].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[samples['relevant_sample'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 random posts side by side as an example\n",
    "html_output = \"\"\n",
    "for _, post in samples.sample(n=3).iterrows():\n",
    "    # Build keywords list from all array fields\n",
    "    keywords = []\n",
    "    \n",
    "    # Add array fields with their values\n",
    "    if 'benefits' in post and isinstance(post['benefits'], list):\n",
    "        keywords.extend(post['benefits'])\n",
    "    if 'downsides' in post and isinstance(post['downsides'], list):\n",
    "        keywords.extend(post['downsides'])\n",
    "    if 'use_cases' in post and isinstance(post['use_cases'], list):\n",
    "        keywords.extend(post['use_cases'])\n",
    "    if 'conditions' in post and isinstance(post['conditions'], list):\n",
    "        keywords.extend(post['conditions'])\n",
    "    if 'provider_problems' in post and isinstance(post['provider_problems'], list):\n",
    "        keywords.extend(post['provider_problems'])\n",
    "        \n",
    "    # Add boolean fields as keywords if True\n",
    "    if post.get('seeing_provider'):\n",
    "        keywords.append('Currently seeing provider')\n",
    "    if post.get('previous_provider'):\n",
    "        keywords.append('Has previous provider')\n",
    "        \n",
    "    # Add sentiment if present\n",
    "    if 'sentiment' in post and pd.notna(post['sentiment']):\n",
    "        keywords.append(f'Sentiment: {post[\"sentiment\"]}/10')\n",
    "\n",
    "    html_output += format_post(\n",
    "        subreddit=post['subreddit'],\n",
    "        title=post['title'],\n",
    "        text=post['text'],\n",
    "        # score=post.get('score'),\n",
    "        url=post.get('url'),\n",
    "        keywords=keywords\n",
    "    )\n",
    "\n",
    "from IPython.display import HTML\n",
    "display(HTML(html_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, column, title, x_label, save=True):\n",
    "    values = data[column].values\n",
    "    percentiles = np.percentile(values, [0, 25, 50, 75, 90, 95, 97.5, 99, 99.9, 100])\n",
    "    bucket_edges = np.unique([0, 1] + [int(p) for p in percentiles[1:]])\n",
    "    \n",
    "    bucket_labels = []\n",
    "    for i in range(len(bucket_edges)-1):\n",
    "        if bucket_edges[i+1] == bucket_edges[i]:\n",
    "            continue\n",
    "        if bucket_edges[i+1] == bucket_edges[i] + 1:\n",
    "            bucket_labels.append(str(bucket_edges[i]))\n",
    "        else:\n",
    "            bucket_labels.append(f'{bucket_edges[i]}-{bucket_edges[i+1]-1}')\n",
    "    \n",
    "    bucketed_values = pd.cut(values, bins=bucket_edges, labels=bucket_labels, right=False)\n",
    "    value_counts = bucketed_values.value_counts().sort_index()\n",
    "    \n",
    "    fig = px.bar(x=value_counts.index, y=value_counts.values,\n",
    "                 title=title,\n",
    "                 labels={'x': x_label, 'y': 'Number of Posts'})\n",
    "    \n",
    "    fig.update_layout(\n",
    "        bargap=0.2,\n",
    "        xaxis_title=x_label,\n",
    "        yaxis_title='Number of Posts'\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "        fig.write_image(f\"plots/{title.lower().replace(' ', '_')}.png\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def format_post(subreddit, title, text, score=None, url=None, keywords=None):\n",
    "    html = f\"\"\"\n",
    "    <div style=\"border: 1px solid #ddd; border-radius: 8px; padding: 15px; margin: 10px 5px; background-color: #f9f9f9; display: inline-block; vertical-align: top; width: 30%;\">\n",
    "        <div style=\"color: #666; font-size: 0.9em; margin-bottom: 5px;\">\n",
    "            <a href=\"https://reddit.com/r/{subreddit}\" target=\"_blank\" style=\"text-decoration: none; color: inherit;\">r/{subreddit}</a>\n",
    "        </div>\n",
    "        <div style=\"color: #333; font-size: 1.1em; font-weight: bold; margin-bottom: 10px;\">\n",
    "            <a href=\"{url if url else '#'}\" target=\"_blank\" style=\"text-decoration: none; color: inherit;\">{title}</a>\n",
    "        </div>\n",
    "        {'<div style=\"color: #1a0dab; margin-bottom: 10px;\">Score: ' + str(score) + '</div>' if score is not None else ''}\n",
    "        {'<div style=\"color: #666; margin-bottom: 10px;\">Keywords: ' + ', '.join(keywords) + '</div>' if keywords else ''}\n",
    "        <div style=\"color: #444; line-height: 1.4; max-height: 200px; overflow-y: auto;\">{text[:500]}...</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "def display_examples_section(title):\n",
    "    display(HTML(f'<div style=\"margin: 10px 0;\"><h4 style=\"color: #333; margin: 0; padding: 5px 0; border-bottom: 1px solid #ccc;\">{title}</h4></div>'))\n",
    "\n",
    "def plot_binary_distribution(df, field_name, title, show_examples=False, save=True):\n",
    "    counts = df[field_name].value_counts()\n",
    "    percentages = (counts / len(df)) * 100\n",
    "    fig = px.pie(values=percentages.values, names=percentages.index, title=title)\n",
    "    fig.update_traces(texttemplate='%{value:.1f}%')\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "        fig.write_image(f\"plots/{title.lower().replace(' ', '_')}.png\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    if show_examples:\n",
    "        for value in counts.index:\n",
    "            display_examples_section(f\"Example Posts for {field_name}\")\n",
    "            html_output = '<div style=\"display: flex; flex-direction: row; justify-content: space-between;\">'\n",
    "            example_posts = df[df[field_name] == value].sample(n=3)\n",
    "            for _, post in example_posts.iterrows():\n",
    "                html_output += format_post(post['subreddit'], post['title'], post['text'], url=post.get('url'))\n",
    "            html_output += \"</div>\"\n",
    "            display(HTML(html_output))\n",
    "\n",
    "def plot_integer_distribution(df, field_name, title, show_examples=False, save=True):\n",
    "    fig = px.histogram(df, x=field_name, title=title, nbins=10)\n",
    "    fig.update_traces(histnorm='percent')\n",
    "    \n",
    "    mean_val = df[field_name].mean()\n",
    "    fig.add_vline(x=mean_val, line_dash=\"dash\", line_color=\"red\",\n",
    "                 annotation_text=f\"Mean: {mean_val:.2f}\",\n",
    "                 annotation_position=\"top right\")\n",
    "    \n",
    "    fig.update_layout(yaxis_title=\"Percent\")\n",
    "    \n",
    "    if save:\n",
    "        if not os.path.exists('plots'):\n",
    "            os.makedirs('plots')\n",
    "        fig.write_image(f\"plots/{title.lower().replace(' ', '_')}.png\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    if show_examples:\n",
    "        display_examples_section(f\"Example Posts with {field_name} Scores\")\n",
    "        html_output = '<div style=\"display: flex; flex-direction: row; justify-content: space-between;\">'\n",
    "        example_posts = df.sample(n=3)\n",
    "        for _, post in example_posts.iterrows():\n",
    "            html_output += format_post(post['subreddit'], post['title'], post['text'], post[field_name], url=post.get('url'))\n",
    "        html_output += \"</div>\"\n",
    "        display(HTML(html_output))\n",
    "\n",
    "def plot_list_field(df, field_name, title, limit=10, show_examples=False, save=True):\n",
    "    all_items = list(itertools.chain(*df[field_name].dropna()))\n",
    "    item_counts = Counter(all_items).most_common(limit)\n",
    "    if item_counts:\n",
    "        df_counts = pd.DataFrame(item_counts, columns=[field_name, 'count'])\n",
    "        total_posts = len(df)\n",
    "        df_counts['percent'] = df_counts['count'].apply(lambda x: (x / total_posts) * 100)\n",
    "        \n",
    "        fig = px.bar(df_counts, x=field_name, y='percent', title=title)\n",
    "        fig.update_layout(yaxis_title=\"Percent of Posts\")\n",
    "        \n",
    "        if save:\n",
    "            if not os.path.exists('plots'):\n",
    "                os.makedirs('plots')\n",
    "            fig.write_image(f\"plots/{title.lower().replace(' ', '_')}.png\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "        if show_examples:\n",
    "            display_examples_section(f\"Example Posts with Keywords\")\n",
    "            html_output = '<div style=\"display: flex; flex-direction: row; justify-content: space-between;\">'\n",
    "            example_posts = df[df[field_name].apply(lambda x: len(x) > 0)].sample(n=3)\n",
    "            for _, post in example_posts.iterrows():\n",
    "                html_output += format_post(\n",
    "                    post['subreddit'], \n",
    "                    post['title'], \n",
    "                    post['text'],\n",
    "                    url=post.get('url'),\n",
    "                    keywords=post[field_name]\n",
    "                )\n",
    "            html_output += \"</div>\"\n",
    "            display(HTML(html_output))\n",
    "\n",
    "def plot_all_fields(df, fields_dict, show_examples=True, save=False):\n",
    "    for field_name, field_info in fields_dict.items():\n",
    "        if field_info['type'] == 'boolean':\n",
    "            plot_binary_distribution(df, field_name, f'Distribution of {field_name}', show_examples=show_examples, save=save)\n",
    "        elif field_info['type'] == 'integer':\n",
    "            plot_integer_distribution(df, field_name, f'{field_name} Distribution', show_examples=show_examples, save=save)\n",
    "        elif field_info['type'] == 'array':\n",
    "            plot_list_field(df, field_name, f'Most Common {field_name.capitalize()}', show_examples=show_examples, save=save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of posts over time\n",
    "posts_over_time = pd.to_datetime(samples['created_utc']).dt.to_period('M').value_counts().sort_index()\n",
    "fig1 = px.line(x=posts_over_time.index.astype(str), y=posts_over_time.values, \n",
    "                title='Distribution of Posts Over Time', labels={'x': 'Week Start Date (MM-DD-YY)', 'y': 'Number of Posts'})\n",
    "fig1.update_xaxes(tickvals=posts_over_time.index.astype(str), ticktext=posts_over_time.index.start_time.strftime('%m-%d-%y'))\n",
    "fig1.show()\n",
    "\n",
    "# Simple histogram of subreddit counts using plotly express, sorted by frequency, showing top 20\n",
    "subreddit_counts = samples['subreddit'].value_counts().head(20)\n",
    "fig = px.histogram(\n",
    "    samples[samples['subreddit'].isin(subreddit_counts.index)], \n",
    "    x='subreddit',\n",
    "    title='Distribution of Posts Across Top 20 Subreddits',\n",
    "    category_orders={\"subreddit\": subreddit_counts.index}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Subreddit\",\n",
    "    yaxis_title=\"Number of Posts\", \n",
    "    xaxis_tickangle=45\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Plot score distribution\n",
    "plot_distribution(samples, 'score', 'Distribution of Post Scores', 'Score Range', save=False)\n",
    "\n",
    "# Plot comment distribution \n",
    "plot_distribution(samples, 'num_comments', 'Distribution of Post Comments', 'Comment Range', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Extracted Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fields based on their types defined in FIELDS\n",
    "# Filter samples to only include relevant posts\n",
    "plot_all_fields(samples, FIELDS, show_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate specific groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples_with_value(samples, field, value):\n",
    "    \"\"\"\n",
    "    Display samples that contain a specific value in a field\n",
    "    \n",
    "    Args:\n",
    "        samples (pd.DataFrame): DataFrame containing the samples\n",
    "        field (str): Name of the field to check (e.g. 'downsides', 'benefits')\n",
    "        value (str): Value to look for in the field\n",
    "    \"\"\"\n",
    "    filtered_samples = samples[samples[field].apply(lambda x: value in x)]\n",
    "\n",
    "    # Create HTML output\n",
    "    html_output = \"<div>\"  # Removed max-width constraint entirely\n",
    "    html_output += f\"<h3>{100*len(filtered_samples)/len(samples):.1f}% of samples mention '{value}' in {field}</h3>\"\n",
    "\n",
    "    for _, sample in filtered_samples[:5].iterrows():\n",
    "        html_output += format_post(title=sample['title'], text=sample['text'], subreddit=sample['subreddit'], url=sample['url'])\n",
    "        \n",
    "    html_output += \"</div>\"\n",
    "    display(HTML(html_output))\n",
    "\n",
    "# Example usage:\n",
    "show_samples_with_value(samples, 'provider_problems', 'expensive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_subreddit_examples(df, subreddit, n=5):\n",
    "    subreddit_posts = df[df['subreddit'] == subreddit].sample(n=n)\n",
    "\n",
    "    html_output = '<div style=\"display: flex; flex-direction: row; justify-content: space-between;\">'\n",
    "    for _, post in subreddit_posts.iterrows():\n",
    "        html_output += format_post(\n",
    "            subreddit=post['subreddit'],\n",
    "            title=post['title'], \n",
    "            text=post['text'],\n",
    "            score=post['score'],\n",
    "            url=post['url']\n",
    "        )\n",
    "    html_output += \"</div>\"\n",
    "    display(HTML(html_output))\n",
    "\n",
    "show_subreddit_examples(samples, 'Healthygamergg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment by use case, ordered by frequency\n",
    "use_cases_df = samples.explode('use_cases')\n",
    "use_case_counts = use_cases_df['use_cases'].value_counts().head(10)\n",
    "use_case_sentiment = use_cases_df.groupby('use_cases')['sentiment'].mean()\n",
    "# Reindex sentiment by frequency order and limit to top 10\n",
    "use_case_sentiment = use_case_sentiment.reindex(use_case_counts.index)\n",
    "\n",
    "fig = px.bar(x=use_case_sentiment.index, \n",
    "             y=use_case_sentiment.values,\n",
    "             title='Average Sentiment Score by Top 10 Use Cases (Ordered by Frequency)',\n",
    "             labels={'x': 'Use Case', 'y': 'Average Sentiment Score'},\n",
    "             text=use_case_counts.values)\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.show()\n",
    "\n",
    "# Plot sentiment by condition, ordered by frequency  \n",
    "conditions_df = samples.explode('conditions')\n",
    "condition_counts = conditions_df['conditions'].value_counts().head(10)\n",
    "condition_sentiment = conditions_df.groupby('conditions')['sentiment'].mean()\n",
    "# Reindex sentiment by frequency order and limit to top 10\n",
    "condition_sentiment = condition_sentiment.reindex(condition_counts.index)\n",
    "\n",
    "fig = px.bar(x=condition_sentiment.index,\n",
    "             y=condition_sentiment.values, \n",
    "             title='Average Sentiment Score by Top 10 Conditions (Ordered by Frequency)',\n",
    "             labels={'x': 'Condition', 'y': 'Average Sentiment Score'},\n",
    "             text=condition_counts.values)\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query top popular subreddits to easily analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10000 subreddits from Reddit API\n",
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Initialize list to store subreddit data\n",
    "subreddit_data = []\n",
    "\n",
    "for subreddit in tqdm(REDDIT.subreddits.popular(limit=10000)):\n",
    "    subreddit_data.append({\n",
    "        'subreddit': subreddit.display_name,\n",
    "        'count': subreddit.subscribers,\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "subreddits_df = pd.DataFrame(subreddit_data)\n",
    "subreddits_df.to_csv('top_subreddits.csv', index=False)\n",
    "print(f\"\\nSaved {len(subreddits_df)} subreddits to top_subreddits.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
